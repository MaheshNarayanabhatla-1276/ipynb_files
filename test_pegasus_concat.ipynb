{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b0ad075a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting python-docx\n",
      "  Downloading python-docx-0.8.11.tar.gz (5.6 MB)\n",
      "Requirement already satisfied: lxml>=2.3.2 in c:\\users\\numam\\anaconda3\\lib\\site-packages (from python-docx) (4.6.3)\n",
      "Building wheels for collected packages: python-docx\n",
      "  Building wheel for python-docx (setup.py): started\n",
      "  Building wheel for python-docx (setup.py): finished with status 'done'\n",
      "  Created wheel for python-docx: filename=python_docx-0.8.11-py3-none-any.whl size=184508 sha256=e62172a5e95edd4c64f49d2d3fecefa842c83af5f31d191ea7c018b08f267411\n",
      "  Stored in directory: c:\\users\\numam\\appdata\\local\\pip\\cache\\wheels\\83\\8b\\7c\\09ae60c42c7ba4ed2dddaf2b8b9186cb105255856d6ed3dba5\n",
      "Successfully built python-docx\n",
      "Installing collected packages: python-docx\n",
      "Successfully installed python-docx-0.8.11\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install python-docx "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5184373b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{november 23, 2022 01:51 pm | updated 01:57 pm ist - mangaluru principal bench of the national green tribunal (ngt) in new delhi has directed karnataka state pollution control board (kspcb) and the state government to frame an action plan for remedial action to prevent environment pollution through sewage and solid waste entering the natural water system in and around mangaluru within a month.: 16.66666666666666, the order was passed on monday, november 21 thereafter, the joint committee, constituted earlier on the direction of the tribunal on april 29, 2022, execute the action plan within a month.: 6.833333333333333, the status of compliance as on january 31, 2023, should be reported to the tribunal latest by february 15, according to the tribunal’s bench comprising chairman adarsh kumar goel, judicial member arun kumar tyagi and expert members a. senthil vel and afroz ahmad.: 7.333333333333337, hearing original application no. 307/2022 taken up suo motu following a report published by the hindu on april 26 titled ‘flow of industrial effluents into phalguni results in fish kill, the tribunal had constituted a joint committee comprising officials from the central and state pollution control boards, fisheries department and the office of the deputy commissioner of dakshina kannada.: 12.499999999999993, the committee was told to make spot visits and file a report on the causes for fish kill and other related matters.: 3.333333333333333, ruling out fish kill was due to flow of industrial effluents into gurupura (phalguni) river near mangaluru, the committee in its october 11, 2022, report flagged off glaring lapses in handling of municipal solid and liquid waste.: 8.500000000000002, solid and liquid waste were flowing through storm water drains and joining the phalguni river thereby polluting the riverine system.: 4.166666666666667, mangaluru city corporation and local bodies around the city were responsible for the flagrant violation of the water (prevention and control of pollution) act.: 4.833333333333333, kspcb failed in its duty the ngt on monday november 21 said, despite serious lapses on the part of civic bodies, the karnataka state pollution control board (kspcb) failed to discharge its statutory functions of fixing accountability of violations by initiating prosecution, stopping polluting activities and fixing responsibility on ‘polluter pays’ principle.: 11.33333333333333, the action plan should fix responsibility for past lapses of mcc, kiadb and other polluters.: 2.8333333333333326, the tribunal posted further hearing on march 14.: 1.8333333333333335}\n",
      "11\n",
      "{delhi recorded its coldest morning of the season so far on monday as the temperature fell to 8.9 degrees celsius, while the air quality remained in the 'very poor' category.: 7.666666666666666, the air quality index (aqi) on monday was recorded at 316.: 3.9999999999999996, an aqi between 201 and 300 is considered poor, 301 and 400 very poor, and 401 and 500 severe.: 5.333333333333333}\n",
      "3\n",
      "hello\n",
      "['november 23, 2022 01:51 pm | updated 01:57 pm ist - mangaluru principal bench of the national green tribunal (ngt) in new delhi has directed karnataka state pollution control board (kspcb) and the state government to frame an action plan for remedial action to prevent environment pollution through sewage and solid waste entering the natural water system in and around mangaluru within a month.', 'hearing original application no. 307/2022 taken up suo motu following a report published by the hindu on april 26 titled ‘flow of industrial effluents into phalguni results in fish kill, the tribunal had constituted a joint committee comprising officials from the central and state pollution control boards, fisheries department and the office of the deputy commissioner of dakshina kannada.', 'kspcb failed in its duty the ngt on monday november 21 said, despite serious lapses on the part of civic bodies, the karnataka state pollution control board (kspcb) failed to discharge its statutory functions of fixing accountability of violations by initiating prosecution, stopping polluting activities and fixing responsibility on ‘polluter pays’ principle.', 'ruling out fish kill was due to flow of industrial effluents into gurupura (phalguni) river near mangaluru, the committee in its october 11, 2022, report flagged off glaring lapses in handling of municipal solid and liquid waste.', 'the status of compliance as on january 31, 2023, should be reported to the tribunal latest by february 15, according to the tribunal’s bench comprising chairman adarsh kumar goel, judicial member arun kumar tyagi and expert members a. senthil vel and afroz ahmad.', 'the order was passed on monday, november 21 thereafter, the joint committee, constituted earlier on the direction of the tribunal on april 29, 2022, execute the action plan within a month.', 'mangaluru city corporation and local bodies around the city were responsible for the flagrant violation of the water (prevention and control of pollution) act.', 'solid and liquid waste were flowing through storm water drains and joining the phalguni river thereby polluting the riverine system.', 'the committee was told to make spot visits and file a report on the causes for fish kill and other related matters.', 'the action plan should fix responsibility for past lapses of mcc, kiadb and other polluters.', 'the tribunal posted further hearing on march 14.', \"delhi recorded its coldest morning of the season so far on monday as the temperature fell to 8.9 degrees celsius, while the air quality remained in the 'very poor' category.\", 'an aqi between 201 and 300 is considered poor, 301 and 400 very poor, and 401 and 500 severe.', 'the air quality index (aqi) on monday was recorded at 316.']\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "651fd5f63fac44cea3fa249932e1b723",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/2.28G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from googletrans import Translator\n",
    "import language_tool_python\n",
    "import PyPDF2\n",
    "import docx\n",
    "import regex as re\n",
    "import string\n",
    "import PyPDF2\n",
    "import spacy\n",
    "import torch\n",
    "from string import punctuation\n",
    "from numba import jit, cuda\n",
    "from deepmultilingualpunctuation import PunctuationModel\n",
    "from newspaper import Article\n",
    "from spacy.lang.en.stop_words import STOP_WORDS\n",
    "from transformers import (\n",
    "    PegasusForConditionalGeneration, PegasusTokenizer, pipeline)\n",
    "from heapq import nlargest\n",
    "#from GPUtil import showUtilization as gpu_usage\n",
    "from numba import cuda\n",
    "\n",
    "\n",
    "# ----------------------------------------------------------------\n",
    "# Translators\n",
    "# ----------------------------------------------------------------\n",
    "\n",
    "\n",
    "def translate(summaryText, lang_code):\n",
    "    translator = Translator()\n",
    "    Transtxt = translator.translate(summaryText, src='en', dest=lang_code)\n",
    "    return Transtxt\n",
    "\n",
    "\n",
    "# ----------------------------------------------------------------\n",
    "#  Convert\n",
    "# ----------------------------------------------------------------\n",
    "def filePDF_extract(filePath):\n",
    "\n",
    "    pdffileobj = open(filePath, 'rb')\n",
    "    pdfreader = PyPDF2.PdfFileReader(pdffileobj)\n",
    "    x = pdfreader.numPages\n",
    "    content = \"\"\n",
    "    RetText = \"\"\n",
    "    for i in range(x):\n",
    "        pageobj = pdfreader.getPage(i)\n",
    "        content = pageobj.extractText()\n",
    "        RetText = RetText + content\n",
    "    return RetText\n",
    "\n",
    "\n",
    "def fileDOCX_extract(filePath):\n",
    "    doc = docx.Document(filePath)\n",
    "    fullText = []\n",
    "    for para in doc.paragraphs:\n",
    "        fullText.append(para.text)\n",
    "    return '\\n'.join(fullText)\n",
    "\n",
    "\n",
    "def fileTXT_extract(filepath):\n",
    "    TextFile = open(filepath, \"r\")\n",
    "    textRead = TextFile.read()\n",
    "    TextFile.close()\n",
    "    return textRead\n",
    "\n",
    "\n",
    "def URL_extract(url,url1):\n",
    "\n",
    "    link_paper = (url)\n",
    "    article = Article(link_paper)\n",
    "    article.download()\n",
    "    article.parse()\n",
    "    input_text = article.text\n",
    "    input_text = ' '.join(input_text.split())\n",
    "    link_paper1 = (url1)\n",
    "    article1 = Article(link_paper1)\n",
    "    article1.download()\n",
    "    article1.parse()\n",
    "    input_text1 = article1.text\n",
    "    input_text1 = ' '.join(input_text1.split())\n",
    "    #total_text=input_text + input_text1\n",
    "    #print(total_text)\n",
    "    #return total_text\n",
    "\n",
    "    clean_text = str(input_text.lower())\n",
    "    nlp = spacy.load('en_core_web_sm')\n",
    "    doc = nlp(clean_text)\n",
    "    per = 1.0\n",
    "    tokens = [token.text for token in doc]\n",
    "\n",
    "    word_frequencies = {}\n",
    "\n",
    "    for word in doc:\n",
    "\n",
    "        if word.text.lower() not in list(STOP_WORDS):\n",
    "\n",
    "            if word.text.lower() not in punctuation:\n",
    "\n",
    "                if word.text not in word_frequencies.keys():\n",
    "\n",
    "                    word_frequencies[word.text] = 1\n",
    "\n",
    "                else:\n",
    "\n",
    "                    word_frequencies[word.text] += 1\n",
    "\n",
    "    max_frequency = max(word_frequencies.values())\n",
    "\n",
    "    for word in word_frequencies.keys():\n",
    "        word_frequencies[word] = word_frequencies[word] / max_frequency\n",
    "\n",
    "    sentence_tokens = [sent for sent in doc.sents]\n",
    "    sentence_scores = {}\n",
    "\n",
    "    for sent in sentence_tokens:\n",
    "\n",
    "        for word in sent:\n",
    "\n",
    "            if word.text.lower() in word_frequencies.keys():\n",
    "\n",
    "                if sent not in sentence_scores.keys():\n",
    "\n",
    "                    sentence_scores[sent] = word_frequencies[word.text.lower()]\n",
    "\n",
    "                else:\n",
    "\n",
    "                    sentence_scores[sent] += word_frequencies[word.text.lower()]\n",
    "    print(sentence_scores)\n",
    "    select_length = int(len(sentence_tokens) * per)\n",
    "    print(select_length)\n",
    "    summary = nlargest(select_length, sentence_scores, key=sentence_scores.get)\n",
    "    # print(summary)\n",
    "    final_summary = [word.text for word in summary]\n",
    "    # print(final_summary)\n",
    "    spacy_summary = ' '.join(final_summary)\n",
    "\n",
    "    clean_text = str(input_text1.lower())\n",
    "    nlp = spacy.load('en_core_web_sm')\n",
    "    doc = nlp(clean_text)\n",
    "    per = 1.0\n",
    "    tokens = [token.text for token in doc]\n",
    "\n",
    "    word_frequencies = {}\n",
    "\n",
    "    for word in doc:\n",
    "\n",
    "        if word.text.lower() not in list(STOP_WORDS):\n",
    "\n",
    "            if word.text.lower() not in punctuation:\n",
    "\n",
    "                if word.text not in word_frequencies.keys():\n",
    "\n",
    "                    word_frequencies[word.text] = 1\n",
    "\n",
    "                else:\n",
    "\n",
    "                    word_frequencies[word.text] += 1\n",
    "\n",
    "    max_frequency = max(word_frequencies.values())\n",
    "\n",
    "    for word in word_frequencies.keys():\n",
    "        word_frequencies[word] = word_frequencies[word] / max_frequency\n",
    "\n",
    "    sentence_tokens = [sent for sent in doc.sents]\n",
    "    sentence_scores = {}\n",
    "\n",
    "    for sent in sentence_tokens:\n",
    "\n",
    "        for word in sent:\n",
    "\n",
    "            if word.text.lower() in word_frequencies.keys():\n",
    "\n",
    "                if sent not in sentence_scores.keys():\n",
    "\n",
    "                    sentence_scores[sent] = word_frequencies[word.text.lower()]\n",
    "\n",
    "                else:\n",
    "\n",
    "                    sentence_scores[sent] += word_frequencies[word.text.lower()]\n",
    "    print(sentence_scores)\n",
    "    select_length = int(len(sentence_tokens) * per)\n",
    "    print(select_length)\n",
    "    summary = nlargest(select_length, sentence_scores, key=sentence_scores.get)\n",
    "    # print(summary)\n",
    "    final_summary1= [word.text for word in summary]\n",
    "    # print(final_summary)\n",
    "    spacy_summary = ' '.join(final_summary)\n",
    "\n",
    "    summ=str(final_summary+final_summary1)\n",
    "    print(\"hello\")\n",
    "    print(summ)\n",
    "    return summ\n",
    "\n",
    "\n",
    "\n",
    "def setGrammer(inputText):\n",
    "    model = PunctuationModel()\n",
    "\n",
    "    inputText = inputText.replace(\"<n>\", \"\")\n",
    "\n",
    "    inputText = model.restore_punctuation(inputText)\n",
    "\n",
    "    my_tool = language_tool_python.LanguageTool('en-US')\n",
    "\n",
    "    inputText = my_tool.correct(inputText)\n",
    "\n",
    "    return inputText\n",
    "\n",
    "\n",
    "def Clean_Text(textforCleanup):\n",
    "    text = re.sub(\"https?:\\/\\/.*[\\r\\n]*\", \"\", textforCleanup)\n",
    "    text = re.sub(\"#\", \"\", text)\n",
    "    punct = set(string.punctuation)\n",
    "    text = \"\".join([ch for ch in text if ch not in punct])\n",
    "    text = text.encode(encoding=\"ascii\", errors=\"ignore\")\n",
    "    text = text.decode()\n",
    "    clean_text = \" \".join([word for word in text.split()])\n",
    "    clean_text = str(clean_text.lower())\n",
    "    return clean_text\n",
    "\n",
    "\n",
    "# ----------------------------------------------------------------\n",
    "# BASIC ALGO\n",
    "# ----------------------------------------------------------------\n",
    "\n",
    "\n",
    "def PegasusModel(spacy_summary):\n",
    "    #torch.cuda.empty_cache()\n",
    "    #torch.cuda.memory_summary(device=None, abbreviated=False)\n",
    "    #freeMemory()\n",
    "    model_name = \"google/pegasus-cnn_dailymail\"\n",
    "\n",
    "    # Load pretrained tokenizer\n",
    "    # device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    device = \"cpu\"\n",
    "\n",
    "    pegasus_tokenizer = PegasusTokenizer.from_pretrained(model_name)\n",
    "\n",
    "    pegasus_model = PegasusForConditionalGeneration.from_pretrained(\n",
    "        model_name).to(device)\n",
    "\n",
    "    # Create tokens\n",
    "\n",
    "    tokens = pegasus_tokenizer(\n",
    "        spacy_summary, truncation=True, padding=\"max_length\", return_tensors=\"pt\").to(device)\n",
    "\n",
    "    encoded_summary = pegasus_model.generate(**tokens)\n",
    "\n",
    "    # Decode summarized text\n",
    "\n",
    "    decoded_summary = pegasus_tokenizer.decode(\n",
    "\n",
    "        encoded_summary[0],\n",
    "\n",
    "        skip_special_tokens=True\n",
    "\n",
    "    )\n",
    "\n",
    "    # print(decoded_summary)\n",
    "\n",
    "    summarizer = pipeline(\n",
    "\n",
    "        \"summarization\",\n",
    "\n",
    "        model=model_name,\n",
    "\n",
    "        tokenizer=pegasus_tokenizer,\n",
    "\n",
    "        framework=\"pt\", truncation=True\n",
    "\n",
    "    )\n",
    "\n",
    "    summary = summarizer(spacy_summary, min_length=30, max_length=150)\n",
    "\n",
    "    summary = summary[0][\"summary_text\"]\n",
    "\n",
    "    summary = summary.replace(\"<n>\", \"\")\n",
    "\n",
    "    summary = summary.replace(\"  \", \"\")\n",
    "\n",
    "\n",
    "    return summary\n",
    "\n",
    "\n",
    "def Spacy(input_text):\n",
    "    clean_text = str(input_text.lower())\n",
    "    nlp = spacy.load('en_core_web_sm')\n",
    "    doc = nlp(clean_text)\n",
    "    per = 1.0\n",
    "    tokens = [token.text for token in doc]\n",
    "\n",
    "    word_frequencies = {}\n",
    "\n",
    "    for word in doc:\n",
    "\n",
    "        if word.text.lower() not in list(STOP_WORDS):\n",
    "\n",
    "            if word.text.lower() not in punctuation:\n",
    "\n",
    "                if word.text not in word_frequencies.keys():\n",
    "\n",
    "                    word_frequencies[word.text] = 1\n",
    "\n",
    "                else:\n",
    "\n",
    "                    word_frequencies[word.text] += 1\n",
    "\n",
    "    max_frequency = max(word_frequencies.values())\n",
    "\n",
    "    for word in word_frequencies.keys():\n",
    "        word_frequencies[word] = word_frequencies[word] / max_frequency\n",
    "\n",
    "    sentence_tokens = [sent for sent in doc.sents]\n",
    "    sentence_scores = {}\n",
    "\n",
    "    for sent in sentence_tokens:\n",
    "\n",
    "        for word in sent:\n",
    "\n",
    "            if word.text.lower() in word_frequencies.keys():\n",
    "\n",
    "                if sent not in sentence_scores.keys():\n",
    "\n",
    "                    sentence_scores[sent] = word_frequencies[word.text.lower()]\n",
    "\n",
    "                else:\n",
    "\n",
    "                    sentence_scores[sent] += word_frequencies[word.text.lower()]\n",
    "    print(sentence_scores)\n",
    "    select_length = int(len(sentence_tokens) * per)\n",
    "    print(select_length)\n",
    "    summary = nlargest(select_length, sentence_scores, key=sentence_scores.get)\n",
    "    # print(summary)\n",
    "    final_summary = [word.text for word in summary]\n",
    "    #print(final_summary)\n",
    "    spacy_summary = ' '.join(final_summary)\n",
    "    print(spacy_summary)\n",
    "\n",
    "    return spacy_summary\n",
    "\n",
    "\n",
    "# def freeMemory():\n",
    "#\n",
    "#     print(\"Initial GPU Usage\")\n",
    "#    # gpu_usage()\n",
    "#\n",
    "#     torch.cuda.empty_cache()\n",
    "#\n",
    "#     cuda.select_device(0)\n",
    "#     cuda.close()\n",
    "#     cuda.select_device(0)\n",
    "#\n",
    "#     print(\"GPU Usage after emptying the cache\")\n",
    "#     #gpu_usage()\n",
    "\n",
    "\n",
    "print(PegasusModel(Clean_Text(URL_extract(\"https://www.thehindu.com/news/cities/Mangalore/ngt-directs-kspcb-to-prevent-river-water-pollution-around-mangaluru-sets-timeline/article66173090.ece\",\"https://economictimes.indiatimes.com/news/india/delhi-air-pollution-aqi-back-in-very-poor-category-construction-and-demolition-work-still-banned/videoshow/95653899.cms\"))))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
